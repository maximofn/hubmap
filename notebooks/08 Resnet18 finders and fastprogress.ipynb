{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTPROGRESS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import subprocess as sp\n",
    "if FASTPROGRESS:\n",
    "    from fastprogress.fastprogress import master_bar, progress_bar\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_BS = False\n",
    "SEARCH_LR = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "division = 300\n",
    "resize_img = 50#500\n",
    "BS_train = 16\n",
    "if SEARCH_LR:\n",
    "    BS_train = int(BS_train / 2)\n",
    "BS_val = BS_train * 4\n",
    "EPOCHS = 400\n",
    "LR = 1e-2\n",
    "NOTEBOOK_NAME = '08_Resnet18_finders_and_fastprogress'\n",
    "VALID_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_images',\n",
       " 'test_images',\n",
       " 'train_annotations',\n",
       " 'train.csv',\n",
       " 'sample_submission.csv',\n",
       " 'test.csv',\n",
       " 'submission.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(\"../\")\n",
    "data_path = path / \"data\"\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = data_path / \"train_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>organ</th>\n",
       "      <th>data_source</th>\n",
       "      <th>img_height</th>\n",
       "      <th>img_width</th>\n",
       "      <th>pixel_size</th>\n",
       "      <th>tissue_thickness</th>\n",
       "      <th>rle</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10044</td>\n",
       "      <td>prostate</td>\n",
       "      <td>HPA</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4</td>\n",
       "      <td>1459676 77 1462675 82 1465674 87 1468673 92 14...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>../data/train_images/10044.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10274</td>\n",
       "      <td>prostate</td>\n",
       "      <td>HPA</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4</td>\n",
       "      <td>715707 2 718705 8 721703 11 724701 18 727692 3...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>../data/train_images/10274.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10392</td>\n",
       "      <td>spleen</td>\n",
       "      <td>HPA</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4</td>\n",
       "      <td>1228631 20 1231629 24 1234624 40 1237623 47 12...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>../data/train_images/10392.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10488</td>\n",
       "      <td>lung</td>\n",
       "      <td>HPA</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4</td>\n",
       "      <td>3446519 15 3449517 17 3452514 20 3455510 24 34...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>../data/train_images/10488.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10610</td>\n",
       "      <td>spleen</td>\n",
       "      <td>HPA</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4</td>\n",
       "      <td>478925 68 481909 87 484893 105 487863 154 4908...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>../data/train_images/10610.tiff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     organ data_source  img_height  img_width  pixel_size  \\\n",
       "0  10044  prostate         HPA        3000       3000         0.4   \n",
       "1  10274  prostate         HPA        3000       3000         0.4   \n",
       "2  10392    spleen         HPA        3000       3000         0.4   \n",
       "3  10488      lung         HPA        3000       3000         0.4   \n",
       "4  10610    spleen         HPA        3000       3000         0.4   \n",
       "\n",
       "   tissue_thickness                                                rle   age  \\\n",
       "0                 4  1459676 77 1462675 82 1465674 87 1468673 92 14...  37.0   \n",
       "1                 4  715707 2 718705 8 721703 11 724701 18 727692 3...  76.0   \n",
       "2                 4  1228631 20 1231629 24 1234624 40 1237623 47 12...  82.0   \n",
       "3                 4  3446519 15 3449517 17 3452514 20 3455510 24 34...  78.0   \n",
       "4                 4  478925 68 481909 87 484893 105 487863 154 4908...  21.0   \n",
       "\n",
       "      sex                             path  \n",
       "0    Male  ../data/train_images/10044.tiff  \n",
       "1    Male  ../data/train_images/10274.tiff  \n",
       "2    Male  ../data/train_images/10392.tiff  \n",
       "3    Male  ../data/train_images/10488.tiff  \n",
       "4  Female  ../data/train_images/10610.tiff  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(data_path / \"train.csv\")\n",
    "train_df['path'] = train_df.id.apply(lambda x: f'{str(train_images_path)}/{x}.tiff')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet 50 UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = 2 # 2 classes\n",
    "number_of_channels = 3\n",
    "\n",
    "def conv3x3_bn(ci, co):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(ci, co, 3, padding=1),\n",
    "        torch.nn.BatchNorm2d(co),\n",
    "        torch.nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "def encoder_conv(ci, co):\n",
    "  return torch.nn.Sequential(\n",
    "        torch.nn.MaxPool2d(2),\n",
    "        conv3x3_bn(ci, co),\n",
    "        conv3x3_bn(co, co),\n",
    "    )\n",
    "\n",
    "class deconv(torch.nn.Module):\n",
    "    def __init__(self, ci, co):\n",
    "        super(deconv, self).__init__()\n",
    "        self.upsample = torch.nn.ConvTranspose2d(ci, co, 2, stride=2)\n",
    "        self.conv1 = conv3x3_bn(ci, co)\n",
    "        self.conv2 = conv3x3_bn(co, co)\n",
    "    \n",
    "    # recibe la salida de la capa anetrior y la salida de la etapa\n",
    "    # correspondiente del encoder\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        diffX = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, (diffX, 0, diffY, 0))\n",
    "        # concatenamos los tensores\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class out_conv(torch.nn.Module):\n",
    "    def __init__(self, ci, co, coo):\n",
    "        super(out_conv, self).__init__()\n",
    "        self.upsample = torch.nn.ConvTranspose2d(ci, co, 2, stride=2)\n",
    "        self.conv = conv3x3_bn(ci, co)\n",
    "        self.final = torch.nn.Conv2d(co, coo, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        diffX = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, (diffX, 0, diffY, 0))\n",
    "        x = self.conv(x1)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "class UNetResnet(torch.nn.Module):\n",
    "    def __init__(self, n_classes=number_of_classes, in_ch=number_of_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)           \n",
    "        if in_ch != 3:\n",
    "          self.encoder.conv1 = torch.nn.Conv2d(in_ch, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        self.deconv1 = deconv(512,256)\n",
    "        self.deconv2 = deconv(256,128)\n",
    "        self.deconv3 = deconv(128,64)\n",
    "        self.out = out_conv(64, 64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x_in = torch.tensor(x.clone().detach())\n",
    "        x_in = x.clone().detach()\n",
    "        x = self.encoder.relu(self.encoder.bn1(self.encoder.conv1(x)))\n",
    "        x1 = self.encoder.layer1(x)\n",
    "        x2 = self.encoder.layer2(x1)\n",
    "        x3 = self.encoder.layer3(x2)\n",
    "        x = self.encoder.layer4(x3)\n",
    "        x = self.deconv1(x, x3)\n",
    "        x = self.deconv2(x, x2)\n",
    "        x = self.deconv3(x, x1)\n",
    "        x = self.out(x, x_in)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice coefficient loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num = pred.size(1)\n",
    "    m1 = pred.view(num, -1).float()  # Flatten\n",
    "    m2 = target.view(num, -1).float()  # Flatten\n",
    "    intersection = (m1 * m2).sum().float()\n",
    "    dice = (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)\n",
    "    dice = dice.item()\n",
    "\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle2mask(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (width,height) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [\n",
    "        np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])\n",
    "    ]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo : hi] = 1\n",
    "    if len(shape) == 3:\n",
    "        img = img.reshape(shape[0], shape[1])\n",
    "    else:\n",
    "        img = img.reshape(shape[0], shape[1])\n",
    "    return img.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, dataframe, n_classes=2, dim=2000, interpolation=cv2.INTER_LANCZOS4):\n",
    "    self.dataframe = dataframe\n",
    "    self.n_classes = n_classes\n",
    "    self.dim = dim\n",
    "    self.interpolation = interpolation\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataframe)\n",
    "\n",
    "  def __getitem__(self, ix):\n",
    "    # Get image path from column 'path' in dataframe\n",
    "    img_path = str(self.dataframe.iloc[ix]['path'])\n",
    "    # Load image\n",
    "    img_cv = cv2.imread(img_path)\n",
    "    img_cv = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
    "    # Resize image\n",
    "    img_cv_res = cv2.resize(img_cv, dsize=(self.dim, self.dim), interpolation=self.interpolation)\n",
    "    # Normalize image\n",
    "    img_cv_res_norm = img_cv_res / 255.0\n",
    "    # Convert to tensor\n",
    "    img_tensor = torch.from_numpy(img_cv_res_norm).float().permute(2, 0, 1)\n",
    "\n",
    "    # Get mask\n",
    "    rle = self.dataframe.iloc[ix]['rle']\n",
    "    mask_cv = rle2mask(rle, img_cv.shape)\n",
    "    # Resize mask\n",
    "    mask_cv_res = cv2.resize(mask_cv, dsize=(self.dim, self.dim), interpolation=self.interpolation)\n",
    "    # One-hot encode mask\n",
    "    mask_oh = np.eye(2)[mask_cv_res.astype(int)].astype(np.float32)\n",
    "    # Convert to tensor\n",
    "    mask_tensor = torch.from_numpy(mask_oh).float().permute(2, 0, 1)\n",
    "    \n",
    "    return img_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había 351 imágenes en el dataset, lo hemos dividido en 280 imágenes de entrenamiento y 71 imágenes de validación\n"
     ]
    }
   ],
   "source": [
    "train_split, val_split = train_test_split(train_df, test_size=VALID_SPLIT, shuffle=True, random_state=42, stratify=train_df['organ'])\n",
    "dataset = {\n",
    "    'train': Dataset(train_split, n_classes=2, dim=resize_img),\n",
    "    'val': Dataset(val_split, n_classes=2, dim=resize_img),\n",
    "}\n",
    "\n",
    "print(f\"Había {len(train_df)} imágenes en el dataset, lo hemos dividido en {len(dataset['train'])} imágenes de entrenamiento y {len(dataset['val'])} imágenes de validación\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=BS_train, shuffle=True, pin_memory=True),\n",
    "    'val': torch.utils.data.DataLoader(dataset['val'], batch_size=BS_val, pin_memory=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_BS:\n",
    "    def get_gpu_memory():\n",
    "        command = \"nvidia-smi --query-gpu=memory.total --format=csv\"\n",
    "        memory_total_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "        memory_total_values = [int(x.split()[0]) for i, x in enumerate(memory_total_info)]\n",
    "\n",
    "        command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "        memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "        memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "\n",
    "        command = \"nvidia-smi --query-gpu=memory.used --format=csv\"\n",
    "        memory_used_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "        memory_used_values = [int(x.split()[0]) for i, x in enumerate(memory_used_info)]\n",
    "        return memory_total_values, memory_free_values, memory_used_values\n",
    "    total, free, used = get_gpu_memory()\n",
    "    print(f\"GPU memory: total: {total} MiB, free: {free} MiB, used: {used} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_BS:\n",
    "    model = UNetResnet()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() >= 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        else:\n",
    "            model.cuda()\n",
    "    else:\n",
    "        print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_BS:\n",
    "    LR = 1e-3\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_BS:\n",
    "    def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "        model.train()\n",
    "        for (imgs, masks) in dataloader:\n",
    "            # X and y to device\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "            # Compute prediction and loss\n",
    "            pred_mask = model(imgs)\n",
    "            loss = criterion(pred_mask, masks)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_BS:\n",
    "    def list_of_posible_batch_sizes(dataset):\n",
    "        batch_sizes = []\n",
    "        batch_size = 1\n",
    "        while batch_size < 2*len(dataset):\n",
    "            batch_sizes.append(batch_size)\n",
    "            batch_size *= 2\n",
    "        batch_sizes.sort(reverse=True)\n",
    "        return batch_sizes\n",
    "\n",
    "    BSs = list_of_posible_batch_sizes(dataset['train'])\n",
    "    BSs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_BS:\n",
    "    for batchsize_train in BSs:\n",
    "        print(f\"batch size: {batchsize_train}\")\n",
    "        train_dl = torch.utils.data.DataLoader(dataset['train'], batch_size=batchsize_train, shuffle=True, pin_memory=True)\n",
    "        epochs = 3\n",
    "        out_of_memory = False\n",
    "        for t in range(epochs):\n",
    "            print(f\"\\tTrain epoch {t} of {epochs}\")\n",
    "            try:\n",
    "                train_loop(train_dl, model, optimizer, optimizer)\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "                out_of_memory = True\n",
    "                break\n",
    "        if out_of_memory == False:\n",
    "            break\n",
    "        print()\n",
    "    print(f\"Done!, bacth size is {batchsize_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_BS:\n",
    "    BS_train = batchsize_train\n",
    "    BS_val = BS_train * 4\n",
    "    print(f\"For {resize_img}x{resize_img} images size, we use batch size of {BS_train} for train and batch size of {BS_val} for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_LR:\n",
    "    def lr_finder(model, train_dl, loss_fn, device, lr_init=1e-8, lr_end=10, increment=4, beta=0.98):\n",
    "        num = len(train_dl)-1 # Numero de lrs que vamos a probar\n",
    "        q = (lr_end/lr_init)**(1/num) # Incremento de lr\n",
    "        lr = lr_init\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        loss = 0\n",
    "        best_loss = 0.\n",
    "        avg_loss = 0.\n",
    "        smoothed_avg_losses = []\n",
    "        lrs = []\n",
    "        for batch, (imgs, masks) in enumerate(train_dl):\n",
    "            # X and y to device\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "            # Update optimizer\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            # Compute prediction and loss\n",
    "            pred_mask = model(imgs)\n",
    "            loss = loss_fn(pred_mask, masks)\n",
    "            avg_loss = beta * avg_loss + (1-beta) * loss.to(\"cpu\").item()\n",
    "            smoothed_loss = avg_loss / (1 - beta**(batch+1))\n",
    "            if increment is not None:\n",
    "                # Se para si el loss se dispara\n",
    "                window = 10\n",
    "                if batch > window and smoothed_avg_losses[-window] > increment * best_loss:\n",
    "                    return lrs, smoothed_avg_losses\n",
    "            \n",
    "            # Se guarda la menor pérdida\n",
    "            if avg_loss < best_loss or batch==0:\n",
    "                best_loss = smoothed_loss\n",
    "            \n",
    "            # Se guardan los datos\n",
    "            smoothed_avg_losses.append(smoothed_loss)\n",
    "            lrs.append(lr)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update lr\n",
    "            lr *= q\n",
    "\n",
    "        return lrs, smoothed_avg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_LR:\n",
    "    model = UNetResnet()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    [lrss, smoothed_avg_losses] = lr_finder(model, train_dl=dataloader['train'], loss_fn = criterion, device=device)\n",
    "\n",
    "    plt.plot(lrss, smoothed_avg_losses)\n",
    "    plt.xlabel(\"lr\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_LR:\n",
    "    LR = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FASTPROGRESS:\n",
    "    def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "        \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "            expects epoch to start from 1.\n",
    "        \"\"\"\n",
    "        x = range(1, epoch+1)\n",
    "        y = np.concatenate((train_loss, valid_loss))\n",
    "        graphs = [[x,train_loss], [x,valid_loss]]\n",
    "        x_margin = 0.2\n",
    "        y_margin = 0.05\n",
    "        x_bounds = [1-x_margin, epochs+x_margin]\n",
    "        y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "        mb.update_graph(graphs, x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, bar, hist, device, mb=None, fastprogress=True):\n",
    "    train_loss, train_dice = [], []\n",
    "    model.train()\n",
    "    for imgs, masks in bar:\n",
    "        # X and y to device\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred_mask = model(imgs)\n",
    "        loss = loss_fn(pred_mask, masks)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update train loss and dice\n",
    "        dice = dice_coeff(pred_mask, masks)\n",
    "        train_loss.append(loss.item())\n",
    "        train_dice.append(dice)\n",
    "        # Update progress bar\n",
    "        if fastprogress:\n",
    "            mb.child.comment = f'loss: {np.mean(train_loss):.5f}, dice {np.mean(train_dice):.5f}'\n",
    "        else:\n",
    "            bar.set_description(f\"\\t\\tloss {np.mean(train_loss):.5f}, dice {np.mean(train_dice):.5f}\")\n",
    "    hist['loss'].append(np.mean(train_loss))\n",
    "    hist['dice'].append(np.mean(train_dice))\n",
    "    return hist\n",
    "\n",
    "def valid_loop(dataloader, model, loss_fn, optimizer, bar, hist, device, mb=None, fastprogress=True):\n",
    "    val_loss, val_dice = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # for imgs, masks in progress_bar(dataloader['val'], parent=mb):\n",
    "        for imgs, masks in bar:\n",
    "            # X and y to device\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            # Compute prediction and loss\n",
    "            pred_mask = model(imgs)\n",
    "            loss = loss_fn(pred_mask, masks)\n",
    "            # Update test loss and dice\n",
    "            dice = dice_coeff(pred_mask, masks)\n",
    "            val_loss.append(loss.item())\n",
    "            val_dice.append(dice)\n",
    "            # Update progress bar\n",
    "            if fastprogress:\n",
    "                mb.child.comment = f'val_loss: {np.mean(val_loss):>5f}, val_dice: {np.mean(val_dice):>5f}'\n",
    "            else:\n",
    "                bar.set_description(f\"\\t\\tval_loss {np.mean(val_loss):.5f} val_dice {np.mean(val_dice):.5f}\")\n",
    "    hist['val_loss'].append(np.mean(val_loss))\n",
    "    hist['val_dice'].append(np.mean(val_dice))\n",
    "    return hist\n",
    "\n",
    "def fit(model, dataloader, device, epochs=100, lr=3e-4, parallel=False):\n",
    "    len_int_epochs = len(str(epochs))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)#, weight_decay=1e-5)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    best_dice = 0.\n",
    "    model_folder = path/'models'\n",
    "    model_full_name = model_folder/ NOTEBOOK_NAME\n",
    "    schedulerOnPlateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max', \n",
    "        factor=0.1, \n",
    "        patience=10, \n",
    "        verbose=True)\n",
    "    if FASTPROGRESS:\n",
    "        mb = master_bar(range(1, epochs+1))\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        if parallel:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model.cuda()\n",
    "    else:\n",
    "        print(\"Let's use CPU!\")\n",
    "        model.to(device)\n",
    "    hist = {'loss': [], 'dice': [], 'val_loss': [], 'val_dice': []}\n",
    "    for epoch in range(epochs):\n",
    "        # if not FASTPROGRESS:\n",
    "        bar = tqdm(dataloader['train'])\n",
    "        hist = train_loop(dataloader, model, criterion, optimizer, bar, hist, device, fastprogress=False)\n",
    "\n",
    "        # if not FASTPROGRESS:\n",
    "        bar = tqdm(dataloader['val'])\n",
    "        hist = valid_loop(dataloader, model, criterion, optimizer, bar, hist, device, fastprogress=False)\n",
    "        \n",
    "        if len_int_epochs == 1:\n",
    "            print(f\"Epoch {(epoch+1):01d}/{epochs:01d} loss {hist['loss'][-1]:.5f} dice {hist['dice'][-1]:.5f} val_loss {hist['val_loss'][-1]:.5f} val_dice {hist['val_dice'][-1]:.5f}\")\n",
    "        elif len_int_epochs == 2:\n",
    "            print(f\"Epoch {(epoch+1):02d}/{epochs:02d} loss {hist['loss'][-1]:.5f} dice {hist['dice'][-1]:.5f} val_loss {hist['val_loss'][-1]:.5f} val_dice {hist['val_dice'][-1]:.5f}\")\n",
    "        elif len_int_epochs == 3:\n",
    "            print(f\"Epoch {(epoch+1):03d}/{epochs:03d} loss {hist['loss'][-1]:.5f} dice {hist['dice'][-1]:.5f} val_loss {hist['val_loss'][-1]:.5f} val_dice {hist['val_dice'][-1]:.5f}\")\n",
    "        elif len_int_epochs == 4:\n",
    "            print(f\"Epoch {(epoch+1):04d}/{epochs:04d} loss {hist['loss'][-1]:.5f} dice {hist['dice'][-1]:.5f} val_loss {hist['val_loss'][-1]:.5f} val_dice {hist['val_dice'][-1]:.5f}\")\n",
    "        \n",
    "        # mb.main_bar.comment = f'epoch: {epoch}+1/{epochs}, loss: {np.mean(train_loss):>7f}, dice: {np.mean(train_dice):>7f},  val_loss {np.mean(val_loss):.7f}, val_dice {np.mean(val_dice):.7f}'\n",
    "        # mb.write(f'epoch: {epoch}, loss: {np.mean(train_loss):>7f}, dice: {np.mean(train_dice):>7f},  val_loss {np.mean(val_loss):.7f}, val_dice {np.mean(val_dice):.7f}')\n",
    "        # plot_loss_update(epoch, epochs, mb, train_loss, val_loss)\n",
    "\n",
    "        if hist['val_dice'][-1] > best_dice:\n",
    "            best_dice = hist['val_dice'][-1]\n",
    "            model_full_path = model_full_name.with_suffix('.pth')\n",
    "            # torch.save(model.state_dict(), model_full_path)\n",
    "            print(f\"New best dice: {best_dice:.5f}\")\n",
    "            # mb.child.comment = f'New best dice: {best_dice:.5f}'\n",
    "        \n",
    "        schedulerOnPlateau.step(hist['val_loss'][-1])\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tloss 0.38208, dice 3.39417: 100%|██████████| 18/18 [00:13<00:00,  1.35it/s]\n",
      "\t\tval_loss 165.00824 test_dice -17.05330: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 loss 0.38208 dice 3.39417 val_loss 165.00824 val_dice -17.05330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tloss 0.27227, dice 6.62641: 100%|██████████| 18/18 [00:13<00:00,  1.35it/s]\n",
      "\t\tval_loss 7.22000 test_dice -151.78926: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 loss 0.27227 dice 6.62641 val_loss 7.22000 val_dice -151.78926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tloss 0.26514, dice 7.14877: 100%|██████████| 18/18 [00:13<00:00,  1.35it/s]\n",
      "\t\tval_loss 1.69115 test_dice 76.67967: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 loss 0.26514 dice 7.14877 val_loss 1.69115 val_dice 76.67967\n",
      "New best dice: 76.67967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not SEARCH_BS and not SEARCH_LR:\n",
    "    model = UNetResnet()\n",
    "    EPOCHS = 3\n",
    "    hist = fit(model, dataloader, device, epochs=EPOCHS, lr=LR, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "       loss      dice  val_loss   val_dice\n",
      "0  0.342260  3.565934  1.472375  20.051750\n",
      "1  0.273516  7.174211  0.290376  17.309454\n",
      "2  0.255532  7.005233  0.269797   6.258324\n",
      "3  0.253462  7.033930  0.432289   1.396149\n",
      "4  0.250229  6.994567  0.466030   1.179413\n"
     ]
    }
   ],
   "source": [
    "if not SEARCH_BS and not SEARCH_LR:\n",
    "    hist_df = pd.DataFrame(hist)\n",
    "    print(len(hist_df))\n",
    "    print(hist_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not SEARCH_BS and not SEARCH_LR:\n",
    "    # plot the training and testing loss\n",
    "    plt.plot(hist['loss'], label='train_loss')\n",
    "    plt.plot(hist['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not SEARCH_BS and not SEARCH_LR:\n",
    "    # plot the training and testing dice\n",
    "    plt.plot(hist['dice'], label='train_dice')\n",
    "    plt.plot(hist['val_dice'], label='val_dice')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/wallabot/Documentos/kaggle/hubmap/notebooks/08 Resnet18 finders and fastprogress.ipynb Celda 36\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wallabot/Documentos/kaggle/hubmap/notebooks/08%20Resnet18%20finders%20and%20fastprogress.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wallabot/Documentos/kaggle/hubmap/notebooks/08%20Resnet18%20finders%20and%20fastprogress.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/wallabot/Documentos/kaggle/hubmap/notebooks/08%20Resnet18%20finders%20and%20fastprogress.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     output \u001b[39m=\u001b[39m model(sample_val_img)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wallabot/Documentos/kaggle/hubmap/notebooks/08%20Resnet18%20finders%20and%20fastprogress.ipynb#X51sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     pred_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wallabot/Documentos/kaggle/hubmap/notebooks/08%20Resnet18%20finders%20and%20fastprogress.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m fig, (ax1, ax2, ax3) \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m30\u001b[39m,\u001b[39m10\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/HuBMAP_HPA/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/wallabot/Documentos/kaggle/hubmap/notebooks/08 Resnet18 finders and fastprogress.ipynb Celda 36\u001b[0m in \u001b[0;36mUNetResnet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wallabot/Documentos/kaggle/hubmap/notebooks/08%20Resnet18%20finders%20and%20fastprogress.ipynb#X51sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wallabot/Documentos/kaggle/hubmap/notebooks/08%20Resnet18%20finders%20and%20fastprogress.ipynb#X51sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39m# x_in = torch.tensor(x.clone().detach())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wallabot/Documentos/kaggle/hubmap/notebooks/08%20Resnet18%20finders%20and%20fastprogress.ipynb#X51sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     x_in \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/wallabot/Documentos/kaggle/hubmap/notebooks/08%20Resnet18%20finders%20and%20fastprogress.ipynb#X51sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wallabot/Documentos/kaggle/hubmap/notebooks/08%20Resnet18%20finders%20and%20fastprogress.ipynb#X51sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mlayer1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wallabot/Documentos/kaggle/hubmap/notebooks/08%20Resnet18%20finders%20and%20fastprogress.ipynb#X51sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mlayer2(x1)\n",
      "File \u001b[0;32m~/anaconda3/envs/HuBMAP_HPA/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/HuBMAP_HPA/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/HuBMAP_HPA/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "if not SEARCH_BS and not SEARCH_LR:\n",
    "    sample_val_img, sample_val_mask = next(iter(dataloader['val']))\n",
    "    one_sample_img = sample_val_img[0]\n",
    "    one_sample_mask = sample_val_mask[0]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(sample_val_img)[0]\n",
    "        pred_mask = torch.argmax(output, axis=0)\n",
    "        \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,10))\n",
    "    ax1.imshow(one_sample_img.permute(1, 2, 0).cpu().numpy())\n",
    "    ax1.set_title('Image')\n",
    "    ax2.imshow(torch.argmax(one_sample_mask, axis=0).cpu().numpy())\n",
    "    ax2.set_title('mask')\n",
    "    ax3.imshow(pred_mask.squeeze().cpu().numpy())\n",
    "    ax3.set_title('pred_mask')\n",
    "    plt.show()\n",
    "    print(f\"output.shape = {output.shape}, pred_mask.shape = {pred_mask.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('HuBMAP_HPA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5908be7129383694b4ff55a3fd43a54e78208ed305dac25c0e93bbd971e0eb3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
